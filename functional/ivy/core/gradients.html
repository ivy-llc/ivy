

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Gradients &mdash; Ivy 1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Ivy
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../why_ivy.html">Why Ivy?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../using_ivy.html">Using Ivy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../training_a_network.html">Training a Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributing.html">Contributing to Ivy</a></li>
</ul>
<p class="caption"><span class="caption-text">Array</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../array/array_api.html">Array api</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/conversions.html">Conversions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/device.html">Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/general.html">General</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/gradients.html">Gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/image.html">Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/linalg.html">Linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/logic.html">Logic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/math.html">Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/meta.html">Meta</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/random.html">Random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../array/reductions.html">Reductions</a></li>
</ul>
<p class="caption"><span class="caption-text">Functional</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../backends.html">Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ivy.html">Ivy</a></li>
</ul>
<p class="caption"><span class="caption-text">Stateful</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../stateful/activations.html">Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../stateful/converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../stateful/initializers.html">Initializers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../stateful/layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../stateful/module.html">Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../stateful/norms.html">Norms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../stateful/optimizers.html">Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../stateful/sequential.html">Sequential</a></li>
</ul>
<p class="caption"><span class="caption-text">Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../ivy"">Ivy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mech"">Ivy mech</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../vision"">Ivy vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../robot"">Ivy robot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../gym"">Ivy gym</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../memory"">Ivy memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../builder"">Ivy builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../models"">Ivy models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ecosystem"">Ivy ecosystem</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Ivy</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
      <li>Gradients</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/functional/ivy/core/gradients.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-ivy.functional.ivy.core.gradients">
<span id="gradients"></span><h1>Gradients<a class="headerlink" href="#module-ivy.functional.ivy.core.gradients" title="Permalink to this headline">¶</a></h1>
<p>Collection of gradient Ivy functions.</p>
<dl class="py class">
<dt id="ivy.functional.ivy.core.gradients.GradientTracking">
<em class="property">class </em><code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">GradientTracking</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">with_grads</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#GradientTracking"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.GradientTracking" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt id="ivy.functional.ivy.core.gradients.GradientTracking.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">with_grads</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#GradientTracking.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.GradientTracking.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.adam_step">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">adam_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dcdws</span></em>, <em class="sig-param"><span class="n">mw</span></em>, <em class="sig-param"><span class="n">vw</span></em>, <em class="sig-param"><span class="n">step</span></em>, <em class="sig-param"><span class="n">beta1</span><span class="o">=</span><span class="default_value">0.9</span></em>, <em class="sig-param"><span class="n">beta2</span><span class="o">=</span><span class="default_value">0.999</span></em>, <em class="sig-param"><span class="n">epsilon</span><span class="o">=</span><span class="default_value">1e-07</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#adam_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.adam_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute adam step delta, given the derivatives of some cost c with respect to ws, using ADAM update.
<a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam">[reference]</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dcdws</strong> (<em>container of arrays</em>) – Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>mw</strong> (<em>container of arrays</em>) – running average of the gradients</p></li>
<li><p><strong>vw</strong> (<em>container of arrays</em>) – running average of second moments of the gradients</p></li>
<li><p><strong>step</strong> (<em>int</em>) – training step</p></li>
<li><p><strong>beta1</strong> (<em>float</em>) – gradient forgetting factor</p></li>
<li><p><strong>beta2</strong> (<em>float</em>) – second moment of gradient forgetting factor</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) – divisor during adam update, preventing division by zero</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The adam step delta.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.adam_update">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">adam_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ws</span></em>, <em class="sig-param"><span class="n">dcdws</span></em>, <em class="sig-param"><span class="n">lr</span></em>, <em class="sig-param"><span class="n">mw_tm1</span></em>, <em class="sig-param"><span class="n">vw_tm1</span></em>, <em class="sig-param"><span class="n">step</span></em>, <em class="sig-param"><span class="n">beta1</span><span class="o">=</span><span class="default_value">0.9</span></em>, <em class="sig-param"><span class="n">beta2</span><span class="o">=</span><span class="default_value">0.999</span></em>, <em class="sig-param"><span class="n">epsilon</span><span class="o">=</span><span class="default_value">1e-07</span></em>, <em class="sig-param"><span class="n">inplace</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stop_gradients</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#adam_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.adam_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with respect to ws, using ADAM update.
<a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam">[reference]</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> (<em>container of variables</em>) – Weights of the function to be updated.</p></li>
<li><p><strong>dcdws</strong> (<em>container of arrays</em>) – Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> (<em>float</em><em> or </em><em>container of layer-wise rates.</em>) – Learning rate(s), the rate(s) at which the weights should be updated relative to the gradient.</p></li>
<li><p><strong>mw_tm1</strong> (<em>container of arrays</em>) – running average of the gradients, from the previous time-step.</p></li>
<li><p><strong>vw_tm1</strong> (<em>container of arrays</em>) – running average of second moments of the gradients, from the previous time-step.</p></li>
<li><p><strong>step</strong> (<em>int</em>) – training step</p></li>
<li><p><strong>beta1</strong> (<em>float</em>) – gradient forgetting factor</p></li>
<li><p><strong>beta2</strong> (<em>float</em>) – second moment of gradient forgetting factor</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) – divisor during adam update, preventing division by zero</p></li>
<li><p><strong>inplace</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to perform the operation inplace, for backends which support inplace variable updates,
and handle gradients behind the scenes such as PyTorch. If the update step should form part of a
computation graph (i.e. higher order optimization), then this should be set to False.
Default is True, provided the backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to stop the gradients of the variables after each gradient step. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The new function weights ws_new, and also new mw and vw, following the adam updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.execute_with_gradients">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">execute_with_gradients</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">func</span></em>, <em class="sig-param"><span class="n">xs</span></em>, <em class="sig-param"><span class="n">retain_grads</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#execute_with_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.execute_with_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Call function func with input of xs variables, and return func first output y, the gradients [dy/dx for x in xs],
and any other function outputs after the returned y value</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>function</em>) – Function for which we compute the gradients of the output with respect to xs input.</p></li>
<li><p><strong>xs</strong> (<em>sequence of variables</em>) – Variables for which to compute the function gradients with respective to.</p></li>
<li><p><strong>retain_grads</strong> (<em>bool</em>) – Whether to retain the gradients of the returned values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the function first output y, the gradients [dy/dx for x in xs], and any other extra function outputs</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.gradient_descent_update">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">gradient_descent_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ws</span></em>, <em class="sig-param"><span class="n">dcdws</span></em>, <em class="sig-param"><span class="n">lr</span></em>, <em class="sig-param"><span class="n">inplace</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stop_gradients</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#gradient_descent_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.gradient_descent_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with respect to ws, [dc/dw for w in ws].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> (<em>Ivy container</em>) – Weights of the function to be updated.</p></li>
<li><p><strong>dcdws</strong> (<em>Ivy container</em>) – Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> (<em>float</em><em> or </em><em>container of layer-wise rates.</em>) – Learning rate(s), the rate(s) at which the weights should be updated relative to the gradient.</p></li>
<li><p><strong>inplace</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to perform the operation inplace, for backends which support inplace variable updates,
and handle gradients behind the scenes such as PyTorch. If the update step should form part of a
computation graph (i.e. higher order optimization), then this should be set to False.
Default is True, provided the backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to stop the gradients of the variables after each gradient step. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The new function weights ws_new, following the gradient descent updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.is_variable">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">is_variable</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">exclusive</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#is_variable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.is_variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Determines whether the input is a variable or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>array</em>) – An ivy array.</p></li>
<li><p><strong>exclusive</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to check if the data type is exclusively a variable, rather than an array.
For frameworks like JAX that do not have exclusive variable types, the function will always return
False if this flag is set, otherwise the check is the same for general arrays. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Boolean, true if x is a trainable variable, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.lamb_update">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">lamb_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ws</span></em>, <em class="sig-param"><span class="n">dcdws</span></em>, <em class="sig-param"><span class="n">lr</span></em>, <em class="sig-param"><span class="n">mw_tm1</span></em>, <em class="sig-param"><span class="n">vw_tm1</span></em>, <em class="sig-param"><span class="n">step</span></em>, <em class="sig-param"><span class="n">beta1</span><span class="o">=</span><span class="default_value">0.9</span></em>, <em class="sig-param"><span class="n">beta2</span><span class="o">=</span><span class="default_value">0.999</span></em>, <em class="sig-param"><span class="n">epsilon</span><span class="o">=</span><span class="default_value">1e-07</span></em>, <em class="sig-param"><span class="n">max_trust_ratio</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">decay_lambda</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">inplace</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stop_gradients</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#lamb_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.lamb_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with respect to ws, [dc/dw for w in ws],
by applying LAMB method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> (<em>container of variables</em>) – Weights of the function to be updated.</p></li>
<li><p><strong>dcdws</strong> (<em>container of arrays</em>) – Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> (<em>float</em><em> or </em><em>container of layer-wise rates.</em>) – Learning rate(s), the rate(s) at which the weights should be updated relative to the gradient.</p></li>
<li><p><strong>mw_tm1</strong> (<em>container of arrays</em>) – running average of the gradients, from the previous time-step.</p></li>
<li><p><strong>vw_tm1</strong> (<em>container of arrays</em>) – running average of second moments of the gradients, from the previous time-step.</p></li>
<li><p><strong>step</strong> (<em>int</em>) – training step</p></li>
<li><p><strong>beta1</strong> (<em>float</em>) – gradient forgetting factor</p></li>
<li><p><strong>beta2</strong> (<em>float</em>) – second moment of gradient forgetting factor</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) – divisor during adam update, preventing division by zero</p></li>
<li><p><strong>max_trust_ratio</strong> (<em>float</em><em>, </em><em>optional</em>) – The maximum value for the trust ratio. Default is 10.</p></li>
<li><p><strong>decay_lambda</strong> (<em>float</em>) – The factor used for weight decay. Default is zero.</p></li>
<li><p><strong>inplace</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to perform the operation inplace, for backends which support inplace variable updates,
and handle gradients behind the scenes such as PyTorch. If the update step should form part of a
computation graph (i.e. higher order optimization), then this should be set to False.
Default is True, provided the backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to stop the gradients of the variables after each gradient step. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The new function weights ws_new, following the LARS updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.lars_update">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">lars_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ws</span></em>, <em class="sig-param"><span class="n">dcdws</span></em>, <em class="sig-param"><span class="n">lr</span></em>, <em class="sig-param"><span class="n">decay_lambda</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">inplace</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stop_gradients</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#lars_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.lars_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with respect to ws, [dc/dw for w in ws],
by applying Layerwise Adaptive Rate Scaling (LARS) method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> (<em>Ivy container</em>) – Weights of the function to be updated.</p></li>
<li><p><strong>dcdws</strong> (<em>Ivy container</em>) – Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> (<em>float</em>) – Learning rate, the rate at which the weights should be updated relative to the gradient.</p></li>
<li><p><strong>decay_lambda</strong> (<em>float</em>) – The factor used for weight decay. Default is zero.</p></li>
<li><p><strong>inplace</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to perform the operation inplace, for backends which support inplace variable updates,
and handle gradients behind the scenes such as PyTorch. If the update step should form part of a
computation graph (i.e. higher order optimization), then this should be set to False.
Default is True, provided the backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to stop the gradients of the variables after each gradient step. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The new function weights ws_new, following the LARS updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.optimizer_update">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">optimizer_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ws</span></em>, <em class="sig-param"><span class="n">effective_grads</span></em>, <em class="sig-param"><span class="n">lr</span></em>, <em class="sig-param"><span class="n">inplace</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stop_gradients</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#optimizer_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.optimizer_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update weights ws of some function, given the true or effective derivatives of some cost c with respect to ws,
[dc/dw for w in ws].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> (<em>Ivy container</em>) – Weights of the function to be updated.</p></li>
<li><p><strong>effective_grads</strong> (<em>Ivy container</em>) – Effective gradients of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> (<em>float</em><em> or </em><em>container of layer-wise rates.</em>) – Learning rate(s), the rate(s) at which the weights should be updated relative to the gradient.</p></li>
<li><p><strong>inplace</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to perform the operation inplace, for backends which support inplace variable updates,
and handle gradients behind the scenes such as PyTorch. If the update step should form part of a
computation graph (i.e. higher order optimization), then this should be set to False.
Default is True, provided the backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to stop the gradients of the variables after each gradient step. Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The new function weights ws_new, following the optimizer updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.set_with_grads">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">set_with_grads</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">with_grads</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#set_with_grads"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.set_with_grads" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.stop_gradient">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">stop_gradient</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">preserve_type</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#stop_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.stop_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Stops gradient computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>array</em>) – Array for which to stop the gradient.</p></li>
<li><p><strong>preserve_type</strong> – Whether to preserve the input type (ivy.Variable or ivy.Array),
otherwise an array is always returned. Default is True.</p></li>
<li><p><strong>preserve_type</strong> – bool, optional</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The same array x, but with no gradient information.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.unset_with_grads">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">unset_with_grads</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#unset_with_grads"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.unset_with_grads" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.variable">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">variable</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#variable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a variable, which supports gradient computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>array</em>) – An ivy array.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An ivy variable, supporting gradient computation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.variable_data">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">variable_data</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#variable_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.variable_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Some backends wrap arrays in a dedicated variable class. For those frameworks, this function returns that wrapped
array. For frameworks which do not have a dedicated variable class, the function returns the data passed in.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>variable</em>) – An ivy variable.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The internal data stored by the variable</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="ivy.functional.ivy.core.gradients.with_grads">
<code class="sig-prename descclassname">ivy.</code><code class="sig-name descname">with_grads</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">with_grads</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/ivy/functional/ivy/core/gradients.html#with_grads"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ivy.functional.ivy.core.gradients.with_grads" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the input dev if provided, otherwise return the global default device.</p>
</dd></dl>

<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Gradients</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gradients/with_grads.html">with_grads</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/set_with_grads.html">set_with_grads</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/unset_with_grads.html">unset_with_grads</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/variable.html">variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/is_variable.html">is_variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/variable_data.html">variable_data</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/stop_gradient.html">stop_gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/execute_with_gradients.html">execute_with_gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/adam_step.html">adam_step</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/optimizer_update.html">optimizer_update</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/gradient_descent_update.html">gradient_descent_update</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/lars_update.html">lars_update</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/adam_update.html">adam_update</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients/lamb_update.html">lamb_update</a></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Ivy Team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>