<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Frameworks &mdash; Ivy 1.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="icon" type="image/png" href="https://github.com/unifyai/unifyai.github.io/blob/master/img/externally_linked/ivy_logo_only.png?raw=true">
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Graph Tracers" href="graph_tracers.html" />
    <link rel="prev" title="Wrapper Frameworks" href="wrapper_frameworks.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Ivy
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../background.html">Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design.html">Design</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../related_work.html">Related Work</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="api_standards.html">API Standards</a></li>
<li class="toctree-l2"><a class="reference internal" href="wrapper_frameworks.html">Wrapper Frameworks</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Frameworks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#matlab-matlab">MATLAB </a></li>
<li class="toctree-l3"><a class="reference internal" href="#scipy-scipy">SciPy </a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch-torch">Torch </a></li>
<li class="toctree-l3"><a class="reference internal" href="#numpy-numpy">NumPy </a></li>
<li class="toctree-l3"><a class="reference internal" href="#scikit-learn-scikit-learn">SciKit Learn </a></li>
<li class="toctree-l3"><a class="reference internal" href="#theano-theano">Theano </a></li>
<li class="toctree-l3"><a class="reference internal" href="#pandas-pandas">Pandas </a></li>
<li class="toctree-l3"><a class="reference internal" href="#julia-julia">Julia </a></li>
<li class="toctree-l3"><a class="reference internal" href="#apache-spark-mllib-apache-spark-mllib">Apache Spark MLlib </a></li>
<li class="toctree-l3"><a class="reference internal" href="#caffe-caffe">Caffe </a></li>
<li class="toctree-l3"><a class="reference internal" href="#chainer-chainer">Chainer </a></li>
<li class="toctree-l3"><a class="reference internal" href="#tensorflow-1-tensorflow-1">TensorFlow 1 </a></li>
<li class="toctree-l3"><a class="reference internal" href="#mxnet-mxnet">MXNet </a></li>
<li class="toctree-l3"><a class="reference internal" href="#cntk-cntk">CNTK </a></li>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-pytorch">PyTorch </a></li>
<li class="toctree-l3"><a class="reference internal" href="#flux-flux">Flux </a></li>
<li class="toctree-l3"><a class="reference internal" href="#jax-jax">JAX </a></li>
<li class="toctree-l3"><a class="reference internal" href="#tensorflow-2-tensorflow-2">TensorFlow 2 </a></li>
<li class="toctree-l3"><a class="reference internal" href="#dex-language-dex-language">DEX Language </a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="graph_tracers.html">Graph Tracers</a></li>
<li class="toctree-l2"><a class="reference internal" href="exchange_formats.html">Exchange Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="compiler_infrastructure.html">Compiler Infrastructure</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_vendor_compiler_frameworks.html">Multi-Vendor Compiler Frameworks</a></li>
<li class="toctree-l2"><a class="reference internal" href="vendor_specific_apis.html">Vendor-Specific APIs</a></li>
<li class="toctree-l2"><a class="reference internal" href="vendor_specific_compilers.html">Vendor-Specific Compilers</a></li>
<li class="toctree-l2"><a class="reference internal" href="ml_unifying_companies.html">ML-Unifying Companies</a></li>
<li class="toctree-l2"><a class="reference internal" href="what_does_ivy_add.html">What does Ivy Add?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../extensions.html">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_dive.html">Deep Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Functions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/experimental.html">Experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/activations.html">Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/constants.html">Constants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/control_flow_ops.html">Control flow ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/creation.html">Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/data_type.html">Data type</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/device.html">Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/elementwise.html">Elementwise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/experimental.html">Experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/general.html">General</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/gradients.html">Gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/linear_algebra.html">Linear algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/manipulation.html">Manipulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/meta.html">Meta</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/nest.html">Nest</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/norms.html">Norms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/random.html">Random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/searching.html">Searching</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/set.html">Set</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/sorting.html">Sorting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/statistical.html">Statistical</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functional/ivy/utility.html">Utility</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../data_classes/container.html">Container</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_classes/array.html">Array</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Framework Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../stateful/optimizers.html">Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stateful/activations.html">Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stateful/layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stateful/module.html">Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stateful/initializers.html">Initializers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stateful/converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stateful/sequential.html">Sequential</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stateful/helpers.html">Helpers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stateful/norms.html">Norms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Nested array</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../nested_array/nested_array.html">Nested array</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Testing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../testing/assertions.html">Assertions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing/available_frameworks.html">Available frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing/function_testing.html">Function testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing/globals.html">Globals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing/hypothesis_helpers.html">Hypothesis helpers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing/structs.html">Structs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing/test_parameter_flags.html">Test parameter flags</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing/testing_helpers.html">Testing helpers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../ivy"">Ivy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mech"">Ivy mech</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../vision"">Ivy vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../robot"">Ivy robot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gym"">Ivy gym</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../memory"">Ivy memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../builder"">Ivy builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models"">Ivy models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ecosystem"">Ivy ecosystem</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Ivy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../related_work.html">Related Work</a> &raquo;</li>
      <li>Frameworks</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/related_work/frameworks.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="frameworks">
<span id="rworks-frameworks"></span><h1>Frameworks<a class="headerlink" href="#frameworks" title="Permalink to this heading"></a></h1>
<p>Here we list some of the most prominent frameworks for array computation.
These are the individual frameworks which the wrapper frameworks mentioned above generally wrap around and abstract.</p>
<a class="reference internal image-reference" href="https://github.com/unifyai/unifyai.github.io/blob/master/img/externally_linked/related_work/frameworks/ml_framework_timeline.png?raw=true"><img alt="https://github.com/unifyai/unifyai.github.io/blob/master/img/externally_linked/related_work/frameworks/ml_framework_timeline.png?raw=true" src="https://github.com/unifyai/unifyai.github.io/blob/master/img/externally_linked/related_work/frameworks/ml_framework_timeline.png?raw=true" style="width: 100%;" /></a>
<section id="matlab-matlab">
<h2>MATLAB <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/matlab.png"><img alt="matlab" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/matlab.png" style="height: 20pt;" /></a><a class="headerlink" href="#matlab-matlab" title="Permalink to this heading"></a></h2>
<p><a class="reference external" href="https://mathworks.com/products/matlab.html">MATLAB</a> (an abbreviation of MATrix LABoratory) is a proprietary multi-paradigm programming language and numeric computing environment developed by <a class="reference external" href="https://mathworks.com/">MathWorks</a>, which was first commercially released in 1984.
It allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages.
Although MATLAB is intended primarily for numeric computing, an optional toolbox uses the <a class="reference external" href="https://mathworks.com/discovery/mupad.html">MuPAD</a> symbolic engine allowing access to symbolic computing abilities.
An additional package, <a class="reference external" href="https://mathworks.com/products/simulink.html">Simulink</a>, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.
As of 2020, MATLAB has more than 4 million users worldwide, who come from various backgrounds of engineering, science, and economics.</p>
</section>
<section id="scipy-scipy">
<h2>SciPy <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/scipy.png"><img alt="scipy" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/scipy.png" style="height: 20pt;" /></a><a class="headerlink" href="#scipy-scipy" title="Permalink to this heading"></a></h2>
<p>First released in 2001, <a class="reference external" href="https://scipy.org/">SciPy</a> is a Python framework used for scientific computing and technical computing, with modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering.
While the user interface is in Python, the backend involves Fortran, Cython, C++ and C for high runtime efficiency.
It is built to work with <a class="reference external" href="https://numpy.org/">NumPy</a> arrays, and provides many user-friendly and efficient numerical routines, such as routines for numerical integration and optimization.</p>
</section>
<section id="torch-torch">
<h2>Torch <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/torch.png"><img alt="torch" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/torch.png" style="height: 20pt;" /></a><a class="headerlink" href="#torch-torch" title="Permalink to this heading"></a></h2>
<p>Initially released in 2002, <a class="reference external" href="http://torch.ch/">Torch</a> is an open-source machine learning library, a scientific computing framework, and a script language based on the Lua programming language.
It provides a wide range of algorithms for deep learning, and uses the scripting language LuaJIT, and an underlying C implementation.
It was created in <a class="reference external" href="https://www.idiap.ch/">IDIAP</a> at <a class="reference external" href="https://www.epfl.ch/">EPFL</a>.</p>
</section>
<section id="numpy-numpy">
<h2>NumPy <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/numpy.png"><img alt="numpy" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/numpy.png" style="height: 20pt;" /></a><a class="headerlink" href="#numpy-numpy" title="Permalink to this heading"></a></h2>
<p>First released in 2005, <a class="reference external" href="https://numpy.org/">NumPy</a> is a Python framework which was created by incorporating features of the competing <a class="reference external" href="https://wiki.python.org/moin/NumArray">Numarray</a> into <a class="reference external" href="https://people.csail.mit.edu/jrennie/python/numeric/">Numeric</a>, with extensive modifications.
NumPy supports large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.
NumPy targets the <a class="reference external" href="https://github.com/python/cpython">CPython</a> reference implementation of Python.
NumPy addresses the absence of compiler optimization partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays.
NumPy arrays are strided views on memory.
It has long been the go-to framework for numeric computing in Python.</p>
</section>
<section id="scikit-learn-scikit-learn">
<h2>SciKit Learn <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/scikit-learn.png"><img alt="scikit-learn" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/scikit-learn.png" style="height: 15pt;" /></a><a class="headerlink" href="#scikit-learn-scikit-learn" title="Permalink to this heading"></a></h2>
<p>First released in 2007, <a class="reference external" href="https://scikit-learn.org/">Scikit-learn</a> is a Python framework which features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries <a class="reference external" href="https://numpy.org/">NumPy</a> and <a class="reference external" href="https://scipy.org/">SciPy</a>.</p>
</section>
<section id="theano-theano">
<h2>Theano <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/theano.png"><img alt="theano" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/theano.png" style="height: 10pt;" /></a><a class="headerlink" href="#theano-theano" title="Permalink to this heading"></a></h2>
<p>Initially released in 2007, <a class="reference external" href="https://github.com/Theano/Theano">Theano</a> is a Python framework which focuses on manipulating and evaluating mathematical expressions, especially matrix-valued ones, with an inbuilt optimizing compiler.
Computations are expressed using a <a class="reference external" href="https://numpy.org/">NumPy</a>-esque syntax and are compiled to run efficiently on either CPU or GPU architectures.
Notably, it includes an extensible graph framework suitable for rapid development of custom operators and symbolic optimizations, and it implements an extensible graph transpilation framework.
It is now being continued under the name <a class="reference external" href="https://github.com/aesara-devs/aesara">Aesara</a>.</p>
</section>
<section id="pandas-pandas">
<h2>Pandas <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/pandas.png"><img alt="pandas" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/pandas.png" style="height: 22pt;" /></a><a class="headerlink" href="#pandas-pandas" title="Permalink to this heading"></a></h2>
<p>Initially released in 2008, <a class="reference external" href="https://pandas.pydata.org/">Pandas</a> is a Python framework which focuses on data manipulation and analysis.
In particular, it offers data structures and operations for manipulating numerical tables and time series.
Key features include: a DataFrame object for data manipulation with integrated indexing, tools for reading and writing data between in-memory data structures and different file formats, and label-based slicing, fancy indexing, and subsetting of large data sets.
It is built upon <a class="reference external" href="https://numpy.org/">NumPy</a>, and as such the library is highly optimized for performance, with critical code paths written in Cython or C.</p>
</section>
<section id="julia-julia">
<h2>Julia <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/julia.png"><img alt="julia" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/julia.png" style="height: 20pt;" /></a><a class="headerlink" href="#julia-julia" title="Permalink to this heading"></a></h2>
<p>Initially released in 2012, <a class="reference external" href="https://julialang.org/">Julia</a> is a high-level, dynamic programming language.
Its features are well suited for numerical analysis and computational science.
Distinctive aspects of Julia’s design include a type system with parametric polymorphism in a dynamic programming language; with multiple dispatch as its core programming paradigm.
Julia supports concurrent, (composable) parallel and distributed computing (with or without using MPI or the built-in corresponding to “OpenMP-style” threads), and direct calling of C and Fortran libraries without glue code.
Julia uses a just-in-time (JIT) compiler that is referred to as “just-ahead-of-time” (JAOT) in the Julia community, as Julia compiles all code (by default) to machine code before running it.
Julia is used extensively by researchers at <a class="reference external" href="https://www.nasa.gov/">NASA</a> and <a class="reference external" href="https://home.cern/">CERN</a>, and it was selected by the <a class="reference external" href="https://clima.caltech.edu/">Climate Modeling Alliance</a> as the sole implementation language for their next generation global climate model, to name a few influential users.</p>
</section>
<section id="apache-spark-mllib-apache-spark-mllib">
<h2>Apache Spark MLlib <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/apache-spark-mllib.png"><img alt="apache-spark-mllib" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/apache-spark-mllib.png" style="height: 20pt;" /></a><a class="headerlink" href="#apache-spark-mllib-apache-spark-mllib" title="Permalink to this heading"></a></h2>
<p>Initially released in 2014, <a class="reference external" href="https://spark.apache.org/">Apache Spark</a> is a unified analytics engine for large-scale data processing, implemented in Scala.
It provides an interface for programming clusters with implicit data parallelism and fault tolerance.
<a class="reference external" href="https://spark.apache.org/mllib/">MLlib</a> is a distributed machine-learning framework on top of Spark Core that, due in large part to the distributed memory-based Spark architecture, is very runtime efficient.
Many common machine learning and statistical algorithms have been implemented and are shipped with MLlib which simplifies large scale machine learning pipelines.
MLlib fits into Spark’s APIs and it also interoperates with <a class="reference external" href="https://numpy.org/">NumPy</a> in Python.</p>
</section>
<section id="caffe-caffe">
<h2>Caffe <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/caffe.png"><img alt="caffe" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/caffe.png" style="height: 10pt;" /></a><a class="headerlink" href="#caffe-caffe" title="Permalink to this heading"></a></h2>
<p>Initially released in 2014, <a class="reference external" href="https://caffe.berkeleyvision.org/">Caffe</a> is highly efficient, but is all in C++ which requires frequent re-compiling during development and testing.
It was also not very easy to quickly throw prototypes together, with C++ being much less forgiving than Python as a front-facing language.
In the last few years, Caffe has been merged into <a class="reference external" href="https://pytorch.org/">PyTorch</a>.</p>
</section>
<section id="chainer-chainer">
<h2>Chainer <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/chainer.png"><img alt="chainer" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/chainer.png" style="height: 20pt;" /></a><a class="headerlink" href="#chainer-chainer" title="Permalink to this heading"></a></h2>
<p>Initially released in 2015, <a class="reference external" href="https://chainer.org/">Chainer</a> is written purely in Python on top of <a class="reference external" href="https://numpy.org/">NumPy</a> and <a class="reference external" href="https://cupy.dev/">CuPy</a>.
It is notable for its early adoption of “define-by-run” scheme, as well as its performance on large scale systems.
In December 2019, <a class="reference external" href="https://www.preferred.jp/">Preferred Networks</a> announced the transition of its development effort from Chainer to <a class="reference external" href="https://pytorch.org/">PyTorch</a> and it will only provide maintenance patches after releasing v7.</p>
</section>
<section id="tensorflow-1-tensorflow-1">
<h2>TensorFlow 1 <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/tensorflow-1.png"><img alt="tensorflow-1" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/tensorflow-1.png" style="height: 20pt;" /></a><a class="headerlink" href="#tensorflow-1-tensorflow-1" title="Permalink to this heading"></a></h2>
<p>Initially released in 2015, <a class="reference external" href="https://www.tensorflow.org/versions/r1.15/api_docs">TensorFlow 1</a> enabled graphs to be defined as chains of Python functions, but the lack of python classes in the model construction process made the code very non-pythonic.
It was hard to create meaningful hierarchical and reusable abstractions.
The computation graph was also hard to debug; intermediate values were not accessible in the Python environment, and had to be explicitly extracted from the graph using a tf.session.
Overall it was easier to get started on projects and prototype more quickly than in <a class="reference external" href="https://caffe.berkeleyvision.org/">Caffe</a> (the most popular ML framework at the time, written in C++), but it was not much easier to debug than Caffe due to the compiled graph which could not be stepped through.
The graph also needed to be fully static, with limited ability for branching in the graph, removing the possibility for pure python control flow to form part of the graph.</p>
</section>
<section id="mxnet-mxnet">
<h2>MXNet <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/mxnet.png"><img alt="mxnet" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/mxnet.png" style="height: 20pt;" /></a><a class="headerlink" href="#mxnet-mxnet" title="Permalink to this heading"></a></h2>
<p>Initially released in 2016, <a class="reference external" href="https://mxnet.apache.org/">MXNet</a> allows users to mix symbolic and imperative programming.
At its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly.
A graph optimization layer on top of that makes symbolic execution fast and memory efficient.
Despite having big industry users such as <a class="reference external" href="https://www.amazon.com/">Amazon</a>, MXNet has not gained significant traction among researchers.</p>
</section>
<section id="cntk-cntk">
<h2>CNTK <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/cntk.png"><img alt="cntk" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/cntk.png" style="height: 20pt;" /></a><a class="headerlink" href="#cntk-cntk" title="Permalink to this heading"></a></h2>
<p>Originally released in 2016, the <a class="reference external" href="https://learn.microsoft.com/en-us/cognitive-toolkit/">Microsoft Cognitive Toolkit (CNTK)</a> is an open-source toolkit for commercial-grade distributed deep learning, written entirely in C++.
It describes neural networks as a series of computational steps via a directed graph.
CNTK allows the user to easily realize and combine popular model types such as feed-forward DNNs, convolutional neural networks (CNNs) and recurrent neural networks (RNNs/LSTMs).
CNTK implements stochastic gradient descent (SGD, error backpropagation) learning with automatic differentiation and parallelization across multiple GPUs and servers.
It is no longer being actively developed, having succumbed to the increasing popularity of the frameworks using Python frontend interfaces.</p>
</section>
<section id="pytorch-pytorch">
<h2>PyTorch <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/pytorch.png"><img alt="pytorch" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/pytorch.png" style="height: 22pt;" /></a><a class="headerlink" href="#pytorch-pytorch" title="Permalink to this heading"></a></h2>
<p><a class="reference external" href="https://pytorch.org/">PyTorch</a> came onto the scene another year later in 2016, which also operated very differently to <a class="reference external" href="https://tensorflow.org/">TensorFlow</a>.
PyTorch operates based on asynchronous scheduling on the target device, without any pre-compilation of the full computation graph on the target device required.
This made it possible to combine asynchronous scheduled efficient kernels with pure Python control flow, and also made it easy to query and monitor the intermediate values in the model, with the boundaries of the “computation graph” having been broken down.
This quickly made it very popular for researchers.
Generally PyTorch is the choice of the ML researcher, ML practitioner and the hobbyist.
PyTorch is very Pythonic, very simple to use, very forgiving, and has a tremendous ecosystem built around it.
No other framework comes close to having anything like the <a class="reference external" href="https://pytorch.org/ecosystem/">PyTorch Ecosystem</a>, with a vast collection of third-party libraries in various important topics for ML research.</p>
</section>
<section id="flux-flux">
<h2>Flux <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/flux.png"><img alt="flux" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/flux.png" style="height: 22pt;" /></a><a class="headerlink" href="#flux-flux" title="Permalink to this heading"></a></h2>
<p><a class="reference external" href="https://fluxml.ai/">Flux</a> is a library for machine learning geared towards high-performance production pipelines, written entirely in the <a class="reference external" href="https://julialang.org/">Julia</a> language.
It comes “batteries-included” with many useful tools built in, whilst still enabling the full power of the Julia language to be leveraged.
It follows a few key principles.
It “does the obvious thing”.
Flux has relatively few explicit APIs for features like regularization or embeddings.
Instead, writing down the mathematical form will work – and be fast.
It is “extensible by default”.
Flux is written to be highly extensible and flexible while being performant.
Extending Flux is as simple as using custom code as part of the desired model - it is all high-level Julia code.
“Performance is key”.
Flux integrates with high-performance AD tools such as <a class="reference external" href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a> for generating fast code.
Flux optimizes both CPU and GPU performance.
Scaling workloads easily to multiple GPUs can be done with the help of Julia’s GPU tooling and projects like <a class="reference external" href="https://github.com/FluxML/DaggerFlux.jl">DaggerFlux.jl</a>.
It “plays nicely with others”.
Flux works well with Julia libraries from data frames and images to differential equation solvers, so it is easy to build complex data processing pipelines that integrate Flux models.
Flux and Julia are not used nearly as much as the Python frameworks, but they are growing in popularity.</p>
</section>
<section id="jax-jax">
<h2>JAX <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/jax.png"><img alt="jax" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/jax.png" style="height: 20pt;" /></a><a class="headerlink" href="#jax-jax" title="Permalink to this heading"></a></h2>
<p>All of the other Python frameworks work well when the aim was “vanilla” first-order optimization of neural networks against a scalar loss function, but are not as flexible when anything more customized is required, including meta learning (learning to learn), higher order loss functions, gradient monitoring, and other important research frontiers involving more customization for the gradients.
These frameworks abstract away most of the gradient computation from the developer, making it hard for them to explicitly track gradients, compute vector gradients such as Jacobians, and/or compute higher order gradients such as Hessians etc.
Initially released in 2018, <a class="reference external" href="https://jax.readthedocs.io/">JAX</a> offers an elegant lightweight fully-functional design, which addresses all of these shortcomings, and uses direct bindings to <a class="reference external" href="https://www.tensorflow.org/xla">XLA</a> for running highly performant code on TPUs.
This gives JAX fundamental advantages over <a class="reference external" href="https://pytorch.org/">PyTorch</a> and <a class="reference external" href="https://tensorflow.org/">TensorFlow</a> in terms of user flexibility, ease of debugging, and user control, but has a higher entry barrier for inexperienced ML users, and despite having fundamental advantages over PyTorch and TensorFlow, it still has a very underdeveloped ecosystem.
JAX is generally the choice of the deeply technical ML researcher, working on deeply customized gradient and optimization schemes, and also the runtime performance fanatic.
JAX is not a framework for beginners, but it offers substantially more control for the people who master it.
You can control how your Jacobians are computed, you can design your own vectorized functions using <code class="code docutils literal notranslate"><span class="pre">vmap</span></code>, and you can make design decisions which ensure you can squeeze out every ounce of performance from your model when running on the TPU.
The ecosystem is evolving but is still in its infancy compared to PyTorch.
As mentioned above, The emergence of libraries such as <a class="reference external" href="https://dm-haiku.readthedocs.io/">Haiku</a> and <a class="reference external" href="https://flax.readthedocs.io/">Flax</a> are lowering the entry barrier somewhat.</p>
</section>
<section id="tensorflow-2-tensorflow-2">
<h2>TensorFlow 2 <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/tensorflow-2.png"><img alt="tensorflow-2" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/tensorflow-2.png" style="height: 20pt;" /></a><a class="headerlink" href="#tensorflow-2-tensorflow-2" title="Permalink to this heading"></a></h2>
<p>With <a class="reference external" href="https://pytorch.org/">PyTorch</a> showing clear advantages and gaining in popularity in the Python ML community, <a class="reference external" href="https://www.tensorflow.org/api_docs">TensorFlow 2</a> was released in 2019 which, like PyTorch, also supported eager execution of the computation graph.
However, because TensorFlow was not an eager framework by design, the eager-mode was very inefficient compared to compiled mode, and therefore was targeted mainly at ease of debugging, rather than a default mode in which TensorFlow should be run.
Without a clear niche, TensorFlow seems to have now focused more on edge and mobile devices, with the introduction of <a class="reference external" href="https://www.tensorflow.org/lite">TensorFlow Lite</a>, making it the go-to for industrial enterprise users looking to deploy ML models.
Overall, TensorFlow 2 is a mature language which has been around in some form since 2015, has already needed to reinvent itself a couple of times, and one of the main advantages currently is the very strong bindings for edge and mobile devices via TensorFlow Lite.
Another advantage is the inertia it has with industrial users who adopted TensorFlow in the past years and haven’t transitioned.</p>
</section>
<section id="dex-language-dex-language">
<h2>DEX Language <a class="reference internal" href="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/dex-language.png"><img alt="dex-language" src="https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/related_work/frameworks/dex-language.png" style="height: 20pt;" /></a><a class="headerlink" href="#dex-language-dex-language" title="Permalink to this heading"></a></h2>
<p>Since 2020, the creator of <a class="reference external" href="https://pytorch.org/">PyTorch</a> (and major <a class="reference external" href="https://jax.readthedocs.io/">JAX</a> contributor) <a class="reference external" href="https://github.com/apaszke">Adam Paszke</a> has stopped working much on either PyTorch and JAX, and has been instead spending his time working on the <a class="reference external" href="https://github.com/google-research/dex-lang">Dex language</a>, which looks to combine the clarity and safety of high-level functional languages with the efficiency and parallelism of low-level numerical languages, avoiding the need to compose primitive bulk-array operations.
They propose an explicit nested indexing style that mirrors application of functions to arguments.
The goal of the project is to explore: type systems for array programming, mathematical program transformations like differentiation and integration, user-directed compilation to parallel hardware, and interactive and incremental numerical programming and visualization.
It is quite early and still in an experimental phase, but this framework would provide hugely significant fundamental improvements over all existing frameworks if it reaches a mature stage of development.
The language is built on top of <a class="reference external" href="https://www.haskell.org/">Haskell</a>.</p>
<p><strong>Round Up</strong></p>
<p>If you have any questions, please feel free to reach out on <a class="reference external" href="https://discord.gg/sXyFF8tDtm">discord</a> in the <a class="reference external" href="https://discord.com/channels/799879767196958751/1034436036371157083">related work channel</a> or in the <a class="reference external" href="https://discord.com/channels/799879767196958751/1034436085587120149">related work forum</a>!</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="wrapper_frameworks.html" class="btn btn-neutral float-left" title="Wrapper Frameworks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="graph_tracers.html" class="btn btn-neutral float-right" title="Graph Tracers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2023, Ivy Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>