{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "file: \"unify_existing_code/0_building_blocks/0_1_compile.ipynb\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 0.1: Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this example, we compile our simple unified `ivy` function `normalize` from the [last notebook](). We then show how this newly compiled `normalize` function exhibits much better runtime performance than the non-compiled version."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::: {#colab-button}\n",
    "[![Open in Colab]({{< var remote_badge.colab >}})](https://colab.research.google.com/github/unifyai/demos/blob/main/{{< meta file >}})\n",
    "[![GitHub]({{< var remote_badge.github >}})](https://github.com/unifyai/demos/blob/main/{{< meta file >}})\n",
    ":::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Firstly, let's pick up where we left off in the [last notebook](), with our unified `normalize` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ivy\n",
    "import torch\n",
    "\n",
    "def normalize(x, mean, std):\n",
    "    return torch.div(torch.sub(x, mean), std)\n",
    "\n",
    "normalize = ivy.unify(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the purpose of illustration, we will use `jax` as our backend framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set ivy's backend to jax\n",
    "ivy.set_backend(\"jax\")\n",
    "\n",
    "# Import jax numpy API\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# create random jax arrays for testing\n",
    "x = jnp.randon.uniform(size=10)\n",
    "mean = jnp.mean(x)\n",
    "std = jnp.std(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As in the previous example, the unified function can be executed like so (in this case it will trigger lazy unification, see the [Lazy vs Eager]() section for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize(x, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When calling this function, all of `ivy`'s function wrapping is included in the call stack of `normalize`, which adds runtime overhead. In general, `ivy.trace_graph` strips any arbitrary function down to its constituent functions in the functional API of the target framework. It will then also be compiled to machine-code if the target framework supports low-level compiling (via functions such as `tf.function`, `torch.jit.script`, `torch.jit.trace`, `torch.compile`, `jax.jit` etc.). The code can be compiled like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comp = ivy.trace_graph(normalize)  # compiles to jax, due to ivy.set_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The compiled function can be executed in exactly the same manner as the non-compiled function (in this case it will also trigger lazy compilation, see the [Lazy vs Eager]() section for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comp(x, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The machine-code compilation can be turned off by setting the argument `low_level = False`, in which case it will simply return a chain of Python function in the functional API of the target framework (in this case JAX). This will still improve the runtime efficiency over the original un-compiled version due to the removal of all `ivy` wrapping overhead, but it will not be as runtime efficient as the low-level compiled version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partial_comp = ivy.trace_graph(normalize, low_level=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Again, the compiled function can be executed in exactly the same manner as the non-compiled function (in this case it will also trigger lazy compilation, see the [Lazy vs Eager]() section for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partial_comp(x, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With all lazy unification and compilation calls now performed (which all increase runtime during the very first call of the function), we can now assess the runtime efficiencies of each function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ivy.time_function(normalize)(x, mean, std)\n",
    "ivy.time_function(partial_comp)(x, mean, std)\n",
    "ivy.time_function(comp)(x, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As expected, we can see that the slowest is `normalize`, which includes all `ivy` wrapping overhead. Next is `partial_comp` which has no wrapping overhead but is still expressed entirely in Python, without compiling to low-level code. The fastest is `comp` because the wrapping overhead is removed and the function is compiled to low-level code for maximal efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Round Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That's it, you can now compile `ivy` code for more efficient inference! However, there are several other important topics to master before you're ready to unify ML code like a pro ðŸ¥·. Next, we'll be learning how to transpile code from one framework to another in a single line of code ðŸ”„"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
