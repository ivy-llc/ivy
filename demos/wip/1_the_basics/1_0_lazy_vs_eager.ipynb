{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "file: \"unify_existing_code/1_the_basics/1_0_lazy_vs_eager.ipynb\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1.0: Lazy vs Eager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`ivy.unify`, `ivy.trace_graph` and `ivy.transpile` can all be performed either eagerly or lazily. All examples in the [Building Blocks]() section are performed *lazily*, which means that the unification, compilation, or transpilation process actually occurs during the first call of the *returned* function. This is because all three of these processes depend on function tracing, which requires function arguments to use for the tracing. Alternatively, the arguments can be provided during the `ivy.unify`, `ivy.trace_graph` or `ivy.transpile` call itself, in which case the process is performed *eagerly*. We show some simple examples for each case below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::: {#colab-button}\n",
    "[![Open in Colab]({{< var remote_badge.colab >}})](https://colab.research.google.com/github/unifyai/demos/blob/main/{{< meta file >}})\n",
    "[![GitHub]({{< var remote_badge.github >}})](https://github.com/unifyai/demos/blob/main/{{< meta file >}})\n",
    ":::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Unify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Consider again this simple `torch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ivy\n",
    "import torch\n",
    "\n",
    "def normalize(x, mean, std):\n",
    "    return torch.div(torch.sub(x, mean), std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's also create the dummy `numpy` arrays as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import NumPy\n",
    "import numpy as np\n",
    "\n",
    "# create random numpy arrays for testing\n",
    "x = np.randon.uniform(size=10)\n",
    "mean = np.mean(x)\n",
    "std = np.std(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's assume that our target framework is `tensorflow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "ivy.set_backend(\"tensorflow\")\n",
    "\n",
    "x = tf.constant(x)\n",
    "mean = tf.constant(mean)\n",
    "std = tf.constant(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the example below, the function is unified *lazily*, which means the first function call will execute slowly, as this is when the unification process actually occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm = ivy.unify(normalize)\n",
    "norm(x, mean, std) # slow, lazy unification\n",
    "norm(x, mean, std) # fast, unified on previous call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However, in the following example the unification occurs *eagerly*, and both function calls will be fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm = ivy.unify(normalize, args=(x, mean, std))\n",
    "norm(x, mean, std) # fast, unified at ivy.unify\n",
    "norm(x, mean, std) # fast, unified at ivy.unify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The same is true for compiling. In the example below, the function is compiled *lazily*, which means the first function call will execute slowly, as this is when the compilation process actually occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_comp = ivy.trace_graph(norm)\n",
    "norm_comp(x, mean, std) # slow, lazy compilation\n",
    "norm_comp(x, mean, std) # fast, compiled on previous call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However, in the following example the compilation occurs *eagerly*, and both function calls will be fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_comp = ivy.trace_graph(norm, args=(x, mean, std))\n",
    "norm_comp(x, mean, std) # fast, compiled at ivy.trace_graph\n",
    "norm_comp(x, mean, std) # fast, compiled at ivy.trace_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Transpile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The same is true for transpiling. In the example below, the function is transpiled *lazily*, which means the first function call will execute slowly, as this is when the transpilation process actually occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "norm_trans = ivy.transpile(normalize)\n",
    "norm_trans(x, mean, std) # slow, lazy transpilation\n",
    "norm_trans(x, mean, std) # fast, transpiled on previous call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However, in the following example the transpilation occurs *eagerly*, and both function calls will be fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_trans = ivy.transpile(normalize, args=(x, mean, std))\n",
    "norm_trans(x, mean, std) # fast, transpiled at ivy.transpile\n",
    "norm_trans(x, mean, std) # fast, transpiled at ivy.transpile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Round Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That's it, you now know the difference between lazy vs eager execution for `ivy.unify`, `ivy.trace_graph` and `ivy.transpile`! However, there are several other important topics to master before you're ready to unify ML code like a pro ðŸ¥·. Next, we'll be learning how the [frameworks are selected](), either inferred from the inputs and the function, specified globally, or specified locally. We'll also learn what the implications are for each of these approaches!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
